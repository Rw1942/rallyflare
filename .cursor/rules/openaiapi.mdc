# GPT-5.1 Developer Guide (REST Only)

Cloudflare Workers • TypeScript • D1 • R2

---

## 1. Authentication (Workers-native)

Always use the Worker’s env for secure API key injection:

```ts
const headers = (env: Env) => ({
  "Authorization": `Bearer ${env.OPENAI_API_KEY}`,
});
```

---

## 2. Basic Chat Completion (Non-Streaming)

> Clean REST, no SDK required. Rally usage: internal helpers, summaries, draft replies, etc.

```ts
export async function chat(env: Env, prompt: string) {
  const body = {
    model: "gpt-5.1",
    input: [{ role: "user", content: prompt }],
    // Optional configuration
    reasoning: { effort: "medium" }, // low, medium, high
    text: { verbosity: "low" },      // low, medium, high
    max_output_tokens: 1000
  };

  const resp = await fetch("https://api.openai.com/v1/responses", {
    method: "POST",
    headers: {
      ...headers(env),
      "Content-Type": "application/json",
    },
    body: JSON.stringify(body),
  });

  const json = await resp.json();
  
  // Parse response from output array
  const messageItem = json.output?.find((item: any) => item.type === "message");
  const contentItem = messageItem?.content?.find((c: any) => c.type === "output_text");
  return contentItem?.text || "";
}

// Usage example:
const answer = await chat(env, "Explain the difference between R2 and D1.");
```

---

## 3. Streaming Responses (Token streaming via SSE)

> For fast UX or R2 file processing, Workers streams tokens beautifully.

```ts
export async function streamChat(env: Env, prompt: string) {
  const resp = await fetch("https://api.openai.com/v1/responses/stream", {
    method: "POST",
    headers: {
      ...headers(env),
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      model: "gpt-5.1",
      input: [{ role: "user", content: prompt }],
      reasoning: { effort: "medium" },
      text: { verbosity: "low" }
    }),
  });

  const reader = resp.body!.getReader();
  const decoder = new TextDecoder();

  let text = "";
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    const chunk = decoder.decode(value);
    text += chunk;
    // Optionally: yield chunk to the client stream
  }
  return text;
}
```

---

## 4. Attachments: Files, PDFs, Images

### 4A. Multipart Helper for Workers

> Workers can build multipart bodies directly—no NPM deps required.

```ts
export function buildMultipart(parts: Record<string, any>) {
  const boundary = "----cf-" + crypto.randomUUID();
  const chunks: (string | File)[] = [];

  for (const [name, value] of Object.entries(parts)) {
    chunks.push(`--${boundary}`);
    if (value instanceof File) {
      chunks.push(
        `Content-Disposition: form-data; name="${name}"; filename="${value.name}"`,
        `Content-Type: ${value.type || "application/octet-stream"}`,
        "",
        value
      );
    } else {
      chunks.push(
        `Content-Disposition: form-data; name="${name}"`,
        "",
        typeof value === "string" ? value : JSON.stringify(value)
      );
    }
  }

  chunks.push(`--${boundary}--`, "");
  return {
    body: new Blob(chunks),
    boundary
  };
}
```

### 4B. R2 File to OpenAI

```ts
// Step 1: Load file from R2
const r2Obj = await env.BUCKET.get(key);
const arrayBuffer = await r2Obj.arrayBuffer();
const file = new File([arrayBuffer], key, {
  type: r2Obj.httpMetadata.contentType || "application/octet-stream"
});

// Step 2: Build multipart payload
const { body, boundary } = buildMultipart({
  model: "gpt-5.1",
  input: JSON.stringify([{ role: "user", content: "Summarize this file." }]),
  reasoning: { effort: "medium" },
  file
});

// Step 3: Send to OpenAI
const resp = await fetch("https://api.openai.com/v1/responses", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${env.OPENAI_API_KEY}`,
    "Content-Type": `multipart/form-data; boundary=${boundary}`
  },
  body
});

const json = await resp.json();
const messageItem = json.output?.find((item: any) => item.type === "message");
const contentItem = messageItem?.content?.find((c: any) => c.type === "output_text");
const text = contentItem?.text || "";
```

---

## 5. Handling Direct User Uploads

> Got a POST with a file? Works the same.

```ts
const form = await request.formData();
const file = form.get("file") as File;

const { body, boundary } = buildMultipart({
  model: "gpt-5.1",
  input: JSON.stringify([{ role: "user", content: "Extract all dates from this document." }]),
  file
});

const resp = await fetch("https://api.openai.com/v1/responses", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${env.OPENAI_API_KEY}`,
    "Content-Type": `multipart/form-data; boundary=${boundary}`
  },
  body
});

const json = await resp.json();
const messageItem = json.output?.find((item: any) => item.type === "message");
const contentItem = messageItem?.content?.find((c: any) => c.type === "output_text");
const text = contentItem?.text || "";
```

---

## 6. Multi-Turn Chat (D1 + GPT-5.1)

> Store each chat round in D1; replay ordered history to the model.

**Schema:**

```sql
CREATE TABLE chat_messages (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  session_id TEXT,
  role TEXT,
  content TEXT,
  ts INTEGER
);
```

**Load & Send:**

```ts
const rows = await env.DB.prepare(
  "SELECT role, content FROM chat_messages WHERE session_id=? ORDER BY id"
).bind(sessionId).all();

const resp = await fetch("https://api.openai.com/v1/responses", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${env.OPENAI_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    model: "gpt-5.1",
    input: rows.map(m => ({
      role: m.role,
      content: m.content
    })),
    reasoning: { effort: "medium" },
    text: { verbosity: "low" }
  })
});

const out = await resp.json();
const messageItem = out.output?.find((item: any) => item.type === "message");
const contentItem = messageItem?.content?.find((c: any) => c.type === "output_text");
const outputText = contentItem?.text || "";

await env.DB.prepare(
  "INSERT INTO chat_messages (session_id, role, content, ts) VALUES (?, ?, ?, ?)"
).bind(sessionId, "assistant", outputText, Date.now()).run();
```

---

## 7. Best Practices for Rally

1. **REST everywhere:** Simpler and snappier.
2. **Stream if possible:** Workers stream natively.
3. **R2 for all binary:** D1 is for metadata, not blobs!
4. **Shorten context:** Summarize threads—don’t replay deep history.
5. **File size checks:** Workers have blob limits; check before send.

---

## 8. Full Example: Worker Endpoint with Optional File

```ts
export const onRequestPost: PagesFunction<Env> = async ({ request, env }) => {
  const form = await request.formData();
  const prompt = form.get("prompt") as string;
  const file = form.get("file") as File | null;

  let payload: any = {
    model: "gpt-5.1",
    input: JSON.stringify([{ role: "user", content: prompt }]),
    reasoning: { effort: "medium" },
    text: { verbosity: "low" }
  };

  let headers: Record<string, string> = {
    "Authorization": `Bearer ${env.OPENAI_API_KEY}`
  };

  let body;

  if (file) {
    const { body: multipart, boundary } = buildMultipart({ ...payload, file });
    body = multipart;
    headers["Content-Type"] = `multipart/form-data; boundary=${boundary}`;
  } else {
    headers["Content-Type"] = "application/json";
    body = JSON.stringify(payload);
  }

  const resp = await fetch("https://api.openai.com/v1/responses", {
    method: "POST",
    headers,
    body
  });

  const out = await resp.json();
  const messageItem = out.output?.find((item: any) => item.type === "message");
  const contentItem = messageItem?.content?.find((c: any) => c.type === "output_text");
  return new Response(contentItem?.text || "", { status: 200 });
};
```

---

**Note:** Use REST endpoints (`/v1/responses`, `/v1/responses/stream`) for all GPT-5.1 work in Rally on Workers—avoid the Node SDK for best cold starts and less config.

---
